<!DOCTYPE html><html lang="en" data-theme="lofi"> <head><!-- Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v5.12.8"><!-- Primary Meta Tags --><title>Product Interaction</title><meta name="title" content="Product Interaction"><meta name="description" content="A computer vision system to detect product interactions in retail CCTV footage using pose estimation, segmentation, and depth analysis. Built during my internship at Tictag."><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://awpbash.github.io/projects/product-interaction/"><meta property="og:title" content="Product Interaction"><meta property="og:description" content="A computer vision system to detect product interactions in retail CCTV footage using pose estimation, segmentation, and depth analysis. Built during my internship at Tictag."><meta property="og:image" content="https://awpbash.github.io/projects/product-interaction/header.png"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://awpbash.github.io/projects/product-interaction/"><meta property="twitter:title" content="Product Interaction"><meta property="twitter:description" content="A computer vision system to detect product interactions in retail CCTV footage using pose estimation, segmentation, and depth analysis. Built during my internship at Tictag."><meta property="twitter:image" content="https://awpbash.github.io/projects/product-interaction/header.png"><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><script type="module" src="/_astro/ClientRouter.astro_astro_type_script_index_0_lang.DZnDNxNb.js"></script><link rel="stylesheet" href="/_astro/aboutme.qBKIHZnl.css">
<style>.time-line-container>div:last-child .education__time>.education__line{display:none}.astro-route-announcer{position:absolute;left:0;top:0;clip:rect(0 0 0 0);clip-path:inset(50%);overflow:hidden;white-space:nowrap;width:1px;height:1px}
</style></head> <body> <div class="bg-base-100 drawer lg:drawer-open"> <input id="my-drawer" type="checkbox" class="drawer-toggle"> <div class="drawer-content bg-base-100"> <div class="sticky lg:hidden top-0 z-30 flex h-16 w-full justify-center bg-opacity-90 backdrop-blur transition-all duration-100 bg-base-100 text-base-content shadow-sm"> <div class="navbar"> <div class="navbar-start"> <label for="my-drawer" class="btn btn-square btn-ghost"> <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" class="inline-block w-5 h-5 stroke-current"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path> </svg> </label> </div> <div class="navbar-center"> <a class="btn btn-ghost normal-case text-xl" href="/">Jun Wei üë∑‚Äç‚ôÇÔ∏è</a> </div> <div class="navbar-end"></div> </div> </div> <div class="md:flex md:justify-center"> <main class="p-6 pt-10 lg:max-w-[900px] max-w-[100vw]">  <main class="md:flex md:justify-center"> <article class="prose prose-lg max-w-[750px] prose-img:mx-auto"> <img src="/projects/product-interaction/header.png" alt="Product Interaction" loading="lazy" decoding="async" fetchpriority="auto" width="750" height="422" class="w-full mb-6"> <div> <h1 class="title my-2 text-4xl font-bold"> Product Interaction  </h1> <p class="text-sm text-gray-500 mb-2">Last updated: Aug 1, 2025</p> <div class="mb-4 flex flex-wrap gap-2"> <span class="badge badge-outline text-sm" key="Computer Vision">Computer Vision</span><span class="badge badge-outline text-sm" key="Deep Learning">Deep Learning</span><span class="badge badge-outline text-sm" key="Retail Insights">Retail Insights</span><span class="badge badge-outline text-sm" key="Video Analytics">Video Analytics</span> </div> </div> <div class="divider my-2"></div>  <h3 id="overview">Overview</h3>
<p>During my internship at <strong>Tictag</strong>, I worked on an advanced <strong>product interaction detection system</strong> designed for retail environments. The goal was to analyze monocular CCTV footage and automatically identify when a shopper <strong>interacts with a product</strong> in a specific Region of Interest (ROI) ‚Äî including whether an item was <strong>picked up</strong>, <strong>put back</strong>, or left untouched.</p>
<p>Instead of relying on basic motion detection, this project leverages <strong>deep learning models</strong>, <strong>pose estimation</strong>, <strong>segmentation</strong>, and <strong>depth perception</strong> to infer intent and behavior, unlocking new potential for <strong>in-store analytics</strong>, <strong>shelf optimization</strong>, and <strong>product placement evaluation</strong>.</p>
<hr>
<h3 id="key-objectives">Key Objectives</h3>
<ul>
<li>Detect human interaction with products using overhead or angled CCTV cameras.</li>
<li>Determine <strong>which ROI</strong> (e.g., product shelf) was interacted with.</li>
<li>Classify interactions as either <strong>addition</strong> or <strong>removal</strong> of products.</li>
<li>Enable <strong>retail analytics</strong> for customer behavior without requiring additional sensors.</li>
</ul>
<hr>
<h3 id="how-it-works">How It Works</h3>
<hr>
<p>To accurately detect product interaction events in retail CCTV footage, we developed a multi-stage vision pipeline that combines object detection, pose estimation, spatial classification, segmentation, frame differencing, and depth analysis.</p>
<h4 id="1-person-detection">1. <strong>Person Detection</strong></h4>
<ul>
<li><strong>Model:</strong> <code>PekingU/rtdetr_r50vd_coco_o365</code></li>
<li>We use RF-DETR, a transformer-based object detector, to detect all human figures in the frame.</li>
<li>This lightweight and efficient detector offers reliable bounding boxes for tracking individuals in crowded environments.</li>
</ul>
<h4 id="2-pose-estimation">2. <strong>Pose Estimation</strong></h4>
<ul>
<li><strong>Model:</strong> <code>usyd-community/vitpose-plus-huge</code></li>
<li>For each detected person, we run ViTPose to extract <strong>17 body keypoints</strong>, including wrists, elbows, shoulders, and more.</li>
<li>Each keypoint includes a (x, y) coordinate and a confidence score.</li>
<li>These keypoints are used to infer body pose and the directionality of arm movement, crucial for detecting interactions.</li>
</ul>
<h4 id="3-interaction-classification">3. <strong>Interaction Classification</strong></h4>
<ul>
<li><strong>Model:</strong> Custom <strong>XGBoost</strong> classifier</li>
<li>We calculate the <strong>normalized relative distance</strong> between each keypoint and the center of each predefined ROI.</li>
<li>These relative features help the model learn spatial interaction patterns, independent of camera position and frame resolution.</li>
<li>The XGBoost model then predicts which (if any) ROI the person is likely interacting with in a given frame.</li>
<li>This approach is <strong>distance-aware</strong> and allows classification of interaction even in cases where arm contact is ambiguous.</li>
</ul>
<h4 id="4-body-segmentation">4. <strong>Body Segmentation</strong></h4>
<ul>
<li><strong>Model:</strong> <code>yolov11m-seg</code></li>
<li>To avoid false positives from moving limbs, we segment out the person‚Äôs body inside each ROI.</li>
<li>This ensures that subsequent change detection steps are only sensitive to object-level changes, not body motion.</li>
</ul>
<h4 id="5-frame-differencing">5. <strong>Frame Differencing</strong></h4>
<ul>
<li><strong>Technique:</strong> OpenCV‚Äôs <strong>BackgroundSubtractorMOG</strong></li>
<li>We use background subtraction on cropped ROI patches <strong>before and after</strong> the predicted interaction to detect pixel-level changes.</li>
<li>If significant visual change is detected, we confirm that an interaction indeed altered the scene.</li>
</ul>
<h4 id="6-depth-estimation">6. <strong>Depth Estimation</strong></h4>
<ul>
<li><strong>Model:</strong> <code>Intel/dpt-large</code></li>
<li>To distinguish between <strong>adding</strong> vs <strong>removing</strong> a product, we estimate monocular depth using a DPT model.</li>
<li>For example:
<ul>
<li>If the average depth in the ROI <strong>increases</strong>, it suggests a product was removed (background revealed).</li>
<li>If the average depth <strong>decreases</strong>, it suggests a product was added (foreground occlusion).</li>
</ul>
</li>
<li>This adds a 3D-awareness component to an otherwise 2D pipeline.</li>
</ul>
<hr>
<p>This hybrid rule-based and learning-based system enables fine-grained analysis of retail interactions‚Äîeven under challenging conditions like crowding, occlusion, or suboptimal camera angles.</p>
<hr>
<h3 id="challenges-faced">Challenges Faced</h3>
<p>Building a reliable interaction detection system for retail CCTV analytics was far from straightforward. Some of the key challenges we faced include:</p>
<ul>
<li>
<p><strong>Occlusion:</strong><br>
Customers often block each other or key body parts, especially in crowded retail spaces, making person detection and pose estimation highly unreliable.</p>
</li>
<li>
<p><strong>Crowding and Overlap:</strong><br>
In dense environments, pose keypoints frequently overlapped or were misattributed, leading to confusion in tracking and interaction mapping.</p>
</li>
<li>
<p><strong>Non-Ideal CCTV Angles:</strong><br>
Most models (like ViTPose or YOLO) are trained on datasets with front-facing or side-view images. Retail CCTV footage typically comes from elevated, angled monocular cameras, resulting in reduced accuracy and distorted detections.</p>
</li>
<li>
<p><strong>Misclassification of Actions:</strong><br>
Standing near an ROI or casually moving an arm could be misclassified as an interaction, while real interactions were sometimes missed due to pose ambiguity.</p>
</li>
<li>
<p><strong>Inaccurate Depth from 2D:</strong><br>
With only monocular input, depth estimation was noisy. It was difficult to accurately determine if a hand was entering, hovering over, or leaving an ROI, leading to false positives or negatives.</p>
</li>
<li>
<p><strong>Pose Estimation Jitter:</strong><br>
Wrist and elbow points were especially noisy, fluctuating across frames and making interaction direction estimation unstable.</p>
</li>
<li>
<p><strong>Performance Bottlenecks:</strong><br>
Processing just 1 minute of video could take <strong>over 6 minutes</strong> on an <strong>NVIDIA A100 GPU</strong>, due to the heavy stack of models‚Äî<strong>person detection</strong>, <strong>pose estimation</strong>, <strong>segmentation</strong>, and <strong>depth inference</strong> all run frame-by-frame. This made real-time or near-real-time deployment infeasible without major optimization.</p>
</li>
</ul>
<p>Despite these obstacles, we developed a reasonably robust pipeline by combining multiple vision models and rule-based logic, offering actionable insights for retail clients.</p>
<hr>
<h3 id="demo">Demo</h3>
<p>üö´ <strong>Not Publicly Available</strong><br>
The system is currently used internally at Tictag and cannot be shared due to company policy. Please reach out for a private demonstration.</p>
<p>However, here‚Äôs a representative demo GIF showcasing the interaction detection in action:</p>
<img src="/projects/product-interaction/pid.gif" width="600" alt="Interaction Detection Demo">
<hr>
<h3 id="reflections">Reflections</h3>
<p>This was my first time integrating <strong>multiple AI models</strong> into a real-world deployment pipeline. I learned how to stitch together multiple stages of perception, work with noisy outputs, and build a practical, scalable solution. It also gave me a deeper appreciation for <strong>human-in-the-loop annotation</strong>, something Tictag specializes in.</p>
<hr>
<h3 id="models-used">Models Used</h3>
<ul>
<li><strong>Person Detection:</strong> <code>PekingU/rtdetr_r50vd_coco_o365</code></li>
<li><strong>Pose Estimation:</strong> <code>usyd-community/vitpose-plus-huge</code></li>
<li><strong>Segmentation:</strong> <code>yolov11m-seg</code></li>
<li><strong>Depth Estimation:</strong> <code>Intel/dpt-large</code></li>
</ul>
<hr>
<h3 id="built-with">Built With</h3>
<ul>
<li>Python, OpenCV</li>
<li>ONNX Runtime</li>
<li>XGBoost</li>
<li>Hugging Face Transformers</li>
<li>Tictag‚Äôs custom annotation and video processing platform</li>
</ul>
<hr>
<h3 id="internship--tictag">Internship @ Tictag</h3>
<p>This project was part of my internship at <strong>Tictag</strong>, a startup that builds hybrid AI-human data labeling tools. I worked closely with their engineering and product teams to design, build, and iterate on this interaction detection module. It was an eye-opening experience into the messy but rewarding world of applied computer vision.</p>  </article> </main>  </main> </div> <footer class="footer footer-center block mb-5 pt-10"> <div class="pb-2">
&copy; 2026 Jun Wei
</div> <div class="inline opacity-75">
Developed by <a href="https://manuelernestog.github.io" target="_blank" class="font-bold">Manuel Ernesto</a> using
<!-- Thanks for using this template. You can keep this line to support my work :) --> <a href="https://astrofy-template.netlify.app/" target="_blank" class="font-bold">Astrofy Template ‚ö°Ô∏è</a> </div> </footer>  </div> <div class="drawer-side z-40"> <label for="my-drawer" class="drawer-overlay"></label> <aside class="px-2 pt-2 h-auto min-h-full w-[19rem] bg-base-200 text-base-content flex flex-col"> <div class="w-fit mx-auto mt-5 mb-6"> <a href="/"> <div class="avatar transition ease-in-out hover:scale-[102%] block m-auto"> <div class="w-[8.5rem]"> <img src="/assets/profile.png" alt="Profile image" loading="lazy" decoding="async" fetchpriority="auto" width="300" height="300" class="mask mask-circle"> </div> </div> </a> </div> <ul class="menu grow shrink menu-md overflow-y-auto"> <li><a class="py-3 text-base" id="home" href="/">Home</a></li> <li><a class="py-3 text-base" id="services" href="/aboutme">About Me</a></li> <li><a class="py-3 text-base" id="projects" href="/projects">Projects</a></li> <li><a class="py-3 text-base" id="fitsensei" href="/wip">FitSensei</a></li> <li><a class="py-3 text-base" id="blog" href="/blog">Blog</a></li> <li><a class="py-3 text-base" id="cv" href="/cv">CV</a></li> <li> <a class="py-3 text-base" href="mailto:e0957730@u.nus.edu" target="_blank" referrerpolicy="no-referrer-when-downgrade">Contact</a> </li> </ul> <script>(function(){const sideBarActiveItemID = undefined;
const activeClass = "bg-base-300";

const activeItemElem = document.getElementById(sideBarActiveItemID);
activeItemElem && activeItemElem.classList.add(activeClass);
})();</script> <div class="block sticky pointer-events-none bottom-10 bg-base-200 justify-center h-12 [mask-image:linear-gradient(transparent,#000000)]"></div> <div class="social-icons px-4 pb-5 pt-1 flex self-center justify-center sticky bottom-0 bg-base-200"> <a href="https://manuelernestog.github.io/support-my-work/" target="_blank" class="mx-3" aria-label="Support my work" title="Support my work"> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" style="fill: currentColor;transform: ;msFilter:;"><path fill-rule="evenodd" clip-rule="evenodd" d="M5 2h2v3H5zm4 0h2v3H9zm4 0h2v3h-2zm6 7h-2V7H3v11c0 1.654 1.346 3 3 3h8c1.654 0 3-1.346 3-3h2c1.103 0 2-.897 2-2v-5c0-1.103-.897-2-2-2zm-4 9a1 1 0 0 1-1 1H6a1 1 0 0 1-1-1V9h10v9zm2-2v-5h2l.002 5H17z"></path></svg> </a> <a href="https://github.com/awpbash" target="_blank" class="mx-3" aria-label="Github" title="Github"> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" style="fill: currentColor;transform: ;msFilter:;"><path fill-rule="evenodd" clip-rule="evenodd" d="M12.026 2c-5.509 0-9.974 4.465-9.974 9.974 0 4.406 2.857 8.145 6.821 9.465.499.09.679-.217.679-.481 0-.237-.008-.865-.011-1.696-2.775.602-3.361-1.338-3.361-1.338-.452-1.152-1.107-1.459-1.107-1.459-.905-.619.069-.605.069-.605 1.002.07 1.527 1.028 1.527 1.028.89 1.524 2.336 1.084 2.902.829.091-.645.351-1.085.635-1.334-2.214-.251-4.542-1.107-4.542-4.93 0-1.087.389-1.979 1.024-2.675-.101-.253-.446-1.268.099-2.64 0 0 .837-.269 2.742 1.021a9.582 9.582 0 0 1 2.496-.336 9.554 9.554 0 0 1 2.496.336c1.906-1.291 2.742-1.021 2.742-1.021.545 1.372.203 2.387.099 2.64.64.696 1.024 1.587 1.024 2.675 0 3.833-2.33 4.675-4.552 4.922.355.308.675.916.675 1.846 0 1.334-.012 2.41-.012 2.737 0 .267.178.577.687.479C19.146 20.115 22 16.379 22 11.974 22 6.465 17.535 2 12.026 2z"></path> </svg> </a> <a href="https://www.linkedin.com/in/jun-wei-ng-2b06b6251/" target="_blank" class="mx-3" aria-label="Linkedin" title="Linkedin"> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" style="fill: currentColor;transform: ;msFilter:;"><circle cx="4.983" cy="5.009" r="2.188"></circle><path d="M9.237 8.855v12.139h3.769v-6.003c0-1.584.298-3.118 2.262-3.118 1.937 0 1.961 1.811 1.961 3.218v5.904H21v-6.657c0-3.27-.704-5.783-4.526-5.783-1.835 0-3.065 1.007-3.568 1.96h-.051v-1.66H9.237zm-6.142 0H6.87v12.139H3.095z"></path> </svg> </a> <a href="/rss.xml" target="_blank" class="mx-3" aria-label="RSS Feed" title="RSS Feed"> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" style="fill: currentColor;transform: ;msFilter:;"><path d="M19 20.001C19 11.729 12.271 5 4 5v2c7.168 0 13 5.832 13 13.001h2z"></path><path d="M12 20.001h2C14 14.486 9.514 10 4 10v2c4.411 0 8 3.589 8 8.001z"></path><circle cx="6" cy="18" r="2"></circle> </svg> </a> </div> </aside> </div> </div> </body></html>